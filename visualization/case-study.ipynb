{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inputs\n",
        "expected_load = [600, 1500]\n",
        "expected_latency = [25, 35]\n",
        "expected_availability = [0.90, 1]\n",
        "degraded_load = [1500, 3000]\n",
        "\n",
        "# Problem Definitions\n",
        "case_study_3_problem = {\n",
        "    'num_vars': 4,\n",
        "    'names': ['xLoad', 'zLatency', 'zAvailability', 'newLoad'],\n",
        "    'bounds': [ expected_load, expected_latency, expected_availability, degraded_load],\n",
        "    'simulation':{\n",
        "        'scenario':'load2',\n",
        "        'models':[\"A\", \"B\", \"C\", \"O\"],\n",
        "        #'models':[\"O\"],\n",
        "        'independent':'loadFromX',\n",
        "        'dependent':[\n",
        "            'loadFromX', \n",
        "            'loadFromY', \n",
        "            'meanLatencyFromY',\n",
        "            \"meanAvailabilityFromY\",\n",
        "            \"throughput\",\n",
        "            \"queueSize\"\n",
        "        ]\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saving\n",
            "\t ..\\out\\case-studies\\simulation.json\n",
            "\t ..\\out\\case-studies\\param_values.txt\n"
          ]
        }
      ],
      "source": [
        "from SALib.sample import saltelli\n",
        "import numpy as np\n",
        "from os import path\n",
        "import json\n",
        "\n",
        "# Model Inputs\n",
        "problem = case_study_3_problem\n",
        "\n",
        "# Suggesting N = 1000\n",
        "param_values = saltelli.sample(problem, 512, calc_second_order=False)\n",
        "\n",
        "casestudy_path = path.join(\"..\", \"out\", \"case-studies\")\n",
        "simulation_file_path = path.join(casestudy_path, \"simulation.json\")\n",
        "param_file_path = path.join(casestudy_path, \"param_values.txt\")\n",
        "\n",
        "print(\"saving\")\n",
        "print(\"\\t\", simulation_file_path)\n",
        "print(\"\\t\", param_file_path)\n",
        "\n",
        "np.savetxt(param_file_path, param_values, fmt='%1.4f')\n",
        "with open(simulation_file_path, 'w', encoding=\"utf-8\") as f:\n",
        "  json.dump(problem[\"simulation\"], f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run\n",
        "```\n",
        "npx ts-node src/case-studies/3/index.ts\n",
        "```\n",
        "or with cluster:\n",
        "```\n",
        "npx ts-node src/case-studies/3/cluster_start.ts\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_n(datay, start, end):\n",
        "    if len(datay) < start:\n",
        "        return np.nan\n",
        "    if(len(datay) < end):\n",
        "        return np.nan\n",
        "\n",
        "    return np.mean(datay[start:end])\n",
        "\n",
        "metrics = [\"T10\"]\n",
        "\n",
        "def get_dependent_name_from_output_index(dependents, output_index):\n",
        "    index = output_index // len(metrics)\n",
        "    return dependents[index]\n",
        "\n",
        "def get_output_name(index):\n",
        "    \"\"\"\n",
        "    Gets the corresponding metric name based on the index of the output\n",
        "    \"\"\"\n",
        "    return metrics[index % len(metrics)]\n",
        "\n",
        "def get_dependent_output(a, b):\n",
        "    \"\"\"\n",
        "    a: independent series\n",
        "    b: dependent series\n",
        "    \"\"\"\n",
        "    t10 = mean_n(b, 10, 20)# slice of 20 - 30, corresponding to seconds 10 - 15.\n",
        "    if np.isnan(t10):\n",
        "        print(\"t10 nan\")\n",
        "    return [t10]\n",
        "\n",
        "\n",
        "def get_output(series, independent, dependents, index):\n",
        "    independent_series = series[independent]\n",
        "\n",
        "    result = []\n",
        "    for d in dependents:\n",
        "        dependent_series = series[d]\n",
        "        result += get_dependent_output(independent_series, dependent_series)\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Which simulation results to use, hardcoded because of the disconnect needing to run NodeJS in the previous simulation step\n",
        "time_series_path = path.join(\"..\", \"out\", \"case-studies\", \"results-load2-1631239270290\")\n",
        "\n",
        "with open(path.join(time_series_path, \"simulation.json\")) as json_file:\n",
        "    output_simulation_data = json.load(json_file)\n",
        "\n",
        "independent = output_simulation_data[\"independent\"]\n",
        "dependents = output_simulation_data[\"dependent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: A\n",
            "\tFound 384 files\n",
            "Model: B\n",
            "\tFound 384 files\n",
            "Model: C\n",
            "\tFound 384 files\n",
            "Model: O\n",
            "\tFound 384 files\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output = path.join(time_series_path, model)\n",
        "    time_series_file_name_pattern = path.join(time_series_model_output, \"sim\", \"*.csv\")\n",
        "    all_time_series_csv_files = glob.glob(time_series_file_name_pattern)\n",
        "    print(\"\\tFound\", len(all_time_series_csv_files), \"files\")\n",
        "\n",
        "    # sort by the id so each row still matches the id of the time series\n",
        "    #print(all_time_series_csv_files)\n",
        "    #print([re.findall(\"(\\d+)\\-SteadyLatency\\.csv\",x) for x in all_time_series_csv_files])\n",
        "    sorted_time_series_files = sorted(\n",
        "        all_time_series_csv_files, \n",
        "        key=lambda x:float(re.findall(\"(\\d+)\\-out\\.csv\",x)[0])\n",
        "        )\n",
        "\n",
        "    #print(sorted_time_series_files)\n",
        "    #print([re.findall(\"sim\\\\\\\\\\w-(\\d+)\\-.*\\.csv\",x)[0] for x in all_time_series_csv_files])\n",
        "\n",
        "    time_series = [pd.read_csv(f) for f in sorted_time_series_files]\n",
        "    outputs = [get_output(s, independent, dependents, index) for index, s in enumerate(time_series)]\n",
        "\n",
        "    all_output_file_name = path.join(time_series_model_output, \"output_all_values.txt\")\n",
        "    np.savetxt(all_output_file_name, outputs, fmt='%1.8f')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assuming independent loadFromX and dependents ['loadFromX', 'loadFromY', 'meanLatencyFromY', 'meanAvailabilityFromY', 'throughput', 'queueSize']\n",
            "Model: A\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "Model: B\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "Model: C\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "Model: O\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n",
            "T10\n"
          ]
        }
      ],
      "source": [
        "from SALib.analyze import sobol\n",
        "import matplotlib.pyplot as plot\n",
        "from SALib.plotting.bar import plot as barplot\n",
        "\n",
        "\n",
        "def alert_nan_to_num(o):\n",
        "    f = np.isfinite(o).all()\n",
        "    if not f:\n",
        "        print(\"uh oh\")\n",
        "    return np.nan_to_num(o)\n",
        "\n",
        "# to print Numpy Arrays\n",
        "def default(obj):\n",
        "    if type(obj).__module__ == np.__name__:\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return obj.item()\n",
        "    raise TypeError('Unknown type:', type(obj))\n",
        "\n",
        "print(\"Assuming independent\", independent, \"and dependents\", dependents)\n",
        "\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "\n",
        "    # tranpose to get {number of output} columns of {number of simulations} rows\n",
        "    outputs = np.transpose(np.loadtxt(all_output_file_name, float))\n",
        "\n",
        "    converted = [alert_nan_to_num(o) for o in outputs]\n",
        "    analysis = [sobol.analyze(problem, o, calc_second_order=False) for o in converted]\n",
        "\n",
        "    # save in JSON the FULL FILE including invalid NaN values for historical reasoning\n",
        "    analysis_output_file_name = path.join(time_series_model_output_path, \"analysis.json\")\n",
        "    with open(analysis_output_file_name, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(analysis, f, ensure_ascii=False, indent=4, default=default)\n",
        "\n",
        "    # save only the Total Sensitivity in a csv\n",
        "    total_sensitivities = dict()\n",
        "    for d in dependents:\n",
        "        total_sensitivities[d] = dict()\n",
        "\n",
        "    for index, a in enumerate(analysis):\n",
        "        dependent_name = get_dependent_name_from_output_index(dependents, index)\n",
        "        output_name = get_output_name(index)\n",
        "        total_sensitivities[dependent_name][output_name] = { \"st\":a[\"ST\"], \"conf\":a[\"ST_conf\"] }\n",
        "\n",
        "    sensitivity_output_file_name = path.join(time_series_model_output_path, \"sensitivity.json\")\n",
        "    with open(sensitivity_output_file_name, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(total_sensitivities, f, ensure_ascii=False, indent=4, default=default)\n",
        "    \n",
        "    reduced = dict()\n",
        "    for prop in total_sensitivities:\n",
        "        reduced[prop] = dict()\n",
        "        for proxy in total_sensitivities[prop]:\n",
        "            s = total_sensitivities[prop][proxy]\n",
        "            reduced[prop][proxy] = {\n",
        "                \"st\": s[\"st\"][-1],\n",
        "                \"conf\":s[\"conf\"][-1]\n",
        "            }\n",
        "    sensitivity_reduced_output_file_name = path.join(time_series_model_output_path, \"sensitivity_simple.json\")\n",
        "    with open(sensitivity_reduced_output_file_name, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(reduced, f, ensure_ascii=False, indent=4, default=default)\n",
        "    \n",
        "    # print some quick graphs\n",
        "    for index, a in enumerate(analysis):\n",
        "        dependent_name = get_dependent_name_from_output_index(dependents, index)\n",
        "        output_name = get_output_name(index)\n",
        "\n",
        "        # Manually draw box plots\n",
        "        df = a.to_df()\n",
        "        fig, axes = plot.subplots(1, len(df))\n",
        "        fig.set_tight_layout(True)\n",
        "\n",
        "        fig.suptitle(f\"Sensitivity of {output_name}({index})\")\n",
        "        for idx, f in enumerate(df):\n",
        "            axes[idx] = barplot(f, ax=axes[idx])\n",
        "\n",
        "        img_path = path.join(time_series_model_output_path, f\"sen-{index}-{dependent_name}-{output_name}.png\")\n",
        "        plot.savefig(img_path, facecolor='white', transparent=False)\n",
        "        plot.close(fig)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scatterplot Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: A\n",
            "\t (384, 6)\n",
            "Model: B\n",
            "\t (384, 6)\n",
            "Model: C\n",
            "\t (384, 6)\n",
            "Model: O\n",
            "\t (384, 6)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plot\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "\n",
        "\n",
        "    # tranpose to get {number of output} columns of {number of simulations} rows\n",
        "    outputs = np.loadtxt(all_output_file_name, float)\n",
        "\n",
        "    print(\"\\t\", np.shape(outputs))\n",
        "\n",
        "    num_columns = np.shape(outputs)[1]\n",
        "    num_outputs = len(metrics)\n",
        "    #print(num_columns, num_outputs)\n",
        "    for x in range(0, num_columns, num_outputs):\n",
        "        df = pd.DataFrame(outputs[:, x:x+num_outputs], columns=metrics)\n",
        "        scatter = scatter_matrix(df, alpha = 0.2, figsize = (5, 5), diagonal = 'hist')\n",
        "\n",
        "        d = get_dependent_name_from_output_index(dependents, x)\n",
        "        scatter_file = path.join(time_series_model_output_path, f\"scatter_{d}.png\")\n",
        "        plot.savefig(scatter_file, facecolor='white', transparent=False)\n",
        "        plot.close(\"all\")\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(outputs)\n",
        "    df.columns = [get_dependent_name_from_output_index(dependents, x) for x in range(0, num_columns, num_outputs)]\n",
        "    scatter = scatter_matrix(df, alpha = 0.2, figsize = (30, 30), diagonal = 'hist')\n",
        "    scatter_file = path.join(time_series_model_output_path, \"scatter.png\")\n",
        "    plot.savefig(scatter_file, facecolor='white', transparent=False)\n",
        "    plot.close(\"all\")\n",
        "    #print(\"\\tsaved image to \", scatter_file)\n",
        "    #break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: G\n",
            "\t meanLatencyFromY 0.0\n",
            "\t meanAvailabilityFromY 0.0\n",
            "\t meanResponseGFastLatency 2.4467095907935268e-05\n",
            "\t meanResponseGMediumLatency 1.1445645928011126e-05\n",
            "\t meanResponseGSlowLatency 2.0590155213976543e-05\n",
            "\t meanResponseGFastAvailability 0.0\n",
            "\t meanResponseGMediumAvailability 0.0\n",
            "\t meanResponseGSlowAvailability 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "    outputs = np.loadtxt(all_output_file_name, float)\n",
        "\n",
        "    num_columns = np.shape(outputs)[1]\n",
        "    for x in range(0, num_columns):\n",
        "        df = pd.DataFrame(outputs[:,x])\n",
        "        #print(df)\n",
        "        #print(df.shape)\n",
        "        d = get_dependent_name_from_output_index(dependents, x)\n",
        "        print(\"\\t\", d, df.var()[0])\n",
        "\n",
        "    #print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Case Study 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model in output_simulation_data[\"models\"]:\n",
        "    print(\"Model:\", model)\n",
        "    time_series_model_output_path = path.join(time_series_path, model)\n",
        "    all_output_file_name = path.join(time_series_model_output_path, \"output_all_values.txt\")\n",
        "\n",
        "\n",
        "# Normal Scenario\n",
        "x_norm =      [1.00, 0.92, 0.12, 0.01]\n",
        "xconf_norm =  [0.18, 0.25, 0.06, 0.01]\n",
        "y_norm =      [1.00, 0.96, 0.03, 1.00]\n",
        "yconf_norm =  [0.18, 0.25, 0.01, 0.16]\n",
        "x_norm_label = [1.03, 0.95, 0.12, 0.04]\n",
        "y_norm_label = [1.03, 0.99, 0.06, 1.03]\n",
        "\n",
        "# Degraded\n",
        "x_deg =       [0.84, 0.00, 0.12, 0.01]\n",
        "xconf_deg =   [0.30, 0.00, 0.08, 0.01]\n",
        "y_deg =       [0.94, 0.01, 0.03, 0.88]\n",
        "yconf_deg =   [0.27, 0.01, 0.01, 0.20]\n",
        "x_deg_label = [0.87, 0.00, 0.15, 0.04]\n",
        "y_deg_label = [0.97, 0.04, 0.06, 0.91]\n",
        "\n",
        "x = np.append(x_norm, x_deg)\n",
        "y = np.append(y_norm, y_deg)\n",
        "xconf = np.append(xconf_norm, xconf_deg)\n",
        "yconf = np.append(yconf_norm, yconf_deg)\n",
        "\n",
        "\n",
        "fig, ax = plot.subplots()\n",
        "ax.scatter(x, y)\n",
        "ax.errorbar(x, y, xerr=xconf, yerr=yconf, fmt=\"o\")\n",
        "for i, txt in enumerate(models):\n",
        "  ax.annotate(txt, (x_norm_label[i], y_norm_label[i]))\n",
        "  ax.annotate(txt + \"'\", (x_deg_label[i], y_deg_label[i]))\n",
        "ax.set_xlabel(\"Coupling of Y Latency to Z Latency\")\n",
        "ax.set_ylabel('Coupling of Y Availability to Z Latency')\n",
        "plot.show()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
